{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sq8U3BtmhtRx"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Running Pyspark inÂ Colab**\n",
        "\n",
        "To run spark in Colab, You need proper setup. Next cell will create required setup for running Spark/PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "j0VTrimTI_9O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9_Uz1NL4gHFx"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Create SparkSession from builder\n",
        "# import pyspark\n",
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "#                     .appName('SparkByExamples.com') \\\n",
        "#                     .getOrCreate()"
      ],
      "metadata": {
        "id": "G-yKQ5SnPR7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.version\n",
        "\n"
      ],
      "metadata": {
        "id": "F9xDUEdNPST8",
        "outputId": "efbdbbb3-3c5f-470c-82a8-8ee3fe002619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.sparkContext)\n",
        "print(\"Spark App Name : \"+ spark.sparkContext.appName)"
      ],
      "metadata": {
        "id": "TNElFV1dQ-4s",
        "outputId": "d1dedd0c-0b05-4fce-fc7b-250db3849624",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=pyspark-shell>\n",
            "Spark App Name : pyspark-shell\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# SparkContext stop() method\n",
        "spark.sparkContext.stop()\n"
      ],
      "metadata": {
        "id": "qUp-ux9dQ_Cq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create SparkContext\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"ALY6110_App\")\n",
        "print(sc.appName)\n"
      ],
      "metadata": {
        "id": "MvTVA9ZfQ_Lj",
        "outputId": "baa47f46-ed7f-46c7-ab7a-71ef0600e38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALY6110_App\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create RDD\n",
        "rdd = sc.range(1, 5)\n",
        "print(rdd.collect())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MCwm_xeYQ_PT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4396e02b-f0b1-4ca2-8059-ee9648f6e66d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rdd2 = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n"
      ],
      "metadata": {
        "id": "bl4VfE_OQ_Sg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rddCollect = rdd2.collect()\n",
        "print(\"Number of Partitions: \"+str(rdd2.getNumPartitions()))\n",
        "print(\"Action: First element: \"+str(rdd2.first()))\n",
        "print(rddCollect)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wo2vAXLQSiga",
        "outputId": "a9b568f6-5c4e-4097-e295-d176f07aea23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Partitions: 1\n",
            "Action: First element: 1\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FThcCSO5SikO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HS5wq6n-Sino"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc.stop()"
      ],
      "metadata": {
        "id": "yw3B4yShSiri"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1G6-Cbw_TVJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.stop()"
      ],
      "metadata": {
        "id": "63pv8kkLTVM3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2NZPAF6TZmw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}